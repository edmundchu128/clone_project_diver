{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gcsfs\n",
    "from google.cloud import storage\n",
    "import pyarrow\n",
    "import os\n",
    "from google.oauth2 import service_account\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "from google.cloud import storage,bigquery\n",
    "from flask import jsonify\n",
    "import os\n",
    "# from dotenv import load_dotenv ## for local debugging, comment out when deploy to gcf\n",
    "\n",
    "# load_dotenv() ## for local debugging, comment out when deploy to gcf\n",
    "\n",
    "\n",
    "def transform_centanet_data(event,context):\n",
    "    \"\"\"Triggered by a change to a Cloud Storage bucket.\n",
    "        Args:\n",
    "            event (dict): Event payload.\n",
    "            context (google.cloud.functions.Context): Metadata for the event.\n",
    "        \"\"\"\n",
    "    # Initialize GCF API Response\n",
    "    output_json = {}\n",
    "    \n",
    "    #Initialize functions\n",
    "\n",
    "    def load_data_to_table(project_name: str, dataset_name: str, table_name: str, dataframe_clean: pd.DataFrame) -> str:\n",
    "    ##Directly loads data to table\n",
    "    # Construct a BigQuery client object.\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        table_id = \".\".join(project_name,dataset_name,table_name)\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.PARQUET,\n",
    "        )\n",
    "\n",
    "        load_job = client.load_table_from_dataframe(\n",
    "            dataframe= dataframe_clean, destination=table_id, job_config=job_config\n",
    "        )  # Make an API request.\n",
    "        \n",
    "        result = load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    # Initialize variables\n",
    "    bucket_name = \"project-diver\" #event['bucket']\n",
    "    blob_name = \"STAGING/centanet/2022/centanet_property_transaction_111022.parquet\"#event['name']\n",
    "    directory_path = \"gs://\" + bucket_name + \"/\"\n",
    "    source_file_path = directory_path + blob_name\n",
    "    schema_file_path = directory_path + os.environ.get(\"SCHEMA_FILE_NAME\")\n",
    "\n",
    "    var_year = str(datetime.datetime.utcnow().strftime('%Y'))\n",
    "    var_datetime = str(datetime.datetime.utcnow().strftime('%d%m%y'))\n",
    "    destination_file_path = \"./TRANSFORMED/centanent/{0}/TRANSFORMED_centanet_2022_centanet_property_transaction_{1}.csv\".format(\n",
    "        var_year, var_datetime)\n",
    "\n",
    "    df_raw_property_transaction = pd.read_parquet(source_file_path)\n",
    "\n",
    "    # schema configuration\n",
    "    df_schema = pd.read_csv(schema_file_path)[\n",
    "        [\"Source Column\", \"Target Column\"]].dropna(axis=0)\n",
    "\n",
    "    ## putting the columns into a list - used for extracting \n",
    "    list_of_source_columns = df_schema[\"Source Column\"].to_list()\n",
    "\n",
    "\n",
    "    dict_column_mapping = dict(\n",
    "        zip(df_schema[\"Source Column\"], df_schema[\"Target Column\"]))\n",
    "\n",
    "    df_intermediate = pd.concat([df_raw_property_transaction.drop(['picture', 'scope'], axis=1), df_raw_property_transaction[\"picture\"].apply(\n",
    "        pd.Series), df_raw_property_transaction[\"scope\"].apply(pd.Series)], axis=1)[list_of_source_columns]\n",
    "\n",
    "    df_intermediate = df_intermediate.explode(\"floorPlans\")\n",
    "    df_intermediate = df_intermediate.replace({'postType': {'R': 'Rent',\n",
    "                                                            'S': 'Sale'},\n",
    "                                               'unitType': {'AP': 'Apartment',\n",
    "                                                            'HS': 'House'}})\n",
    "\n",
    "    df_clean = df_intermediate.rename(columns=dict_column_mapping)\n",
    "\n",
    "    \n",
    "\n",
    "    return jsonify(output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GCF API Response\n",
    "output_json = {}\n",
    "\n",
    "#Initialize functions\n",
    "\n",
    "def load_data_to_table(project_name: str, dataset_name: str, table_name: str, dataframe_clean: pd.DataFrame) -> str:\n",
    "##Directly loads data to table##\n",
    "# Construct a BigQuery client object.\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    table_id = \".\".join([dataset_name,table_name])\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.PARQUET,\n",
    "    )\n",
    "\n",
    "    load_job = client.load_table_from_dataframe(\n",
    "        dataframe= dataframe_clean, destination=table_id, job_config=job_config\n",
    "    )  # Make an API request.\n",
    "    \n",
    "    result = load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "    return result\n",
    "    \n",
    "def query_data_from_table(project_name: str, dataset_name: str, table_name: str, query_string: str):\n",
    "    client = bigquery.Client()\n",
    "    query_job = client.query(\n",
    "        query=query_string,\n",
    "    )  # API Request\n",
    "\n",
    "    result = query_job.to_dataframe()# Make the request\n",
    "\n",
    "    return result\n",
    "\n",
    "# Initialize variables\n",
    "bucket_name = \"project-diver\"\n",
    "blob_name = \"STAGING/centanet/2022/centanet_property_transaction_111022.parquet\"\n",
    "directory_path = \"gs://\" + bucket_name + \"/\"\n",
    "source_file_path = \"./DATASETS/STAGING_centanet_2022_centanet_property_transaction_111022.parquet\"#directory_path + blob_name\n",
    "schema_file_path = \"../MODELS/TABLES/property-transaction-table-structure.csv\"#directory_path +\"SCHEMAS/property-transaction-table-structure.csv\"\n",
    "latest_inserted_date = str(query_data_from_table(\"project-diver\",\"project-diver\",\"dbo.property_transaction\",\"SELECT Inserted_Date_Api FROM dbo.property_transaction ORDER BY Inserted_Date_Api DESC LIMIT 1\")[\"Inserted_Date_Api\"][0])\n",
    "\n",
    "var_year = str(datetime.datetime.utcnow().strftime('%Y'))\n",
    "var_datetime = str(datetime.datetime.utcnow().strftime('%d%m%y'))\n",
    "destination_file_path = \"./TRANSFORMED/centanent/{0}/TRANSFORMED_centanet_2022_centanet_property_transaction_{1}.csv\".format(\n",
    "    var_year, var_datetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Read data\n",
    "df_raw_property_transaction = pd.read_parquet(source_file_path)\n",
    "\n",
    "## read schema configuration\n",
    "df_schema = pd.read_csv(schema_file_path)[\n",
    "    [\"Source Column\", \"Target Column\"]].dropna(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=project-diver, location=US, id=ad76e8d9-cdd9-4446-b121-f83bc64109c3>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## putting column names into a list - used for extracting \n",
    "list_of_source_columns = df_schema[\"Source Column\"].to_list()\n",
    "dict_column_mapping = dict(\n",
    "    zip(df_schema[\"Source Column\"], df_schema[\"Target Column\"]))\n",
    "\n",
    "## cleaning data\n",
    "df_intermediate = pd.concat([df_raw_property_transaction.drop(['picture', 'scope'], axis=1), df_raw_property_transaction[\"picture\"].apply(\n",
    "    pd.Series), df_raw_property_transaction[\"scope\"].apply(pd.Series)], axis=1)[list_of_source_columns]\n",
    "df_intermediate['insDate'] = pd.to_datetime(df_intermediate['insDate'])\n",
    "df_intermediate = df_intermediate[(df_intermediate['insDate']>datetime.datetime.strptime(latest_inserted_date,\"%Y-%m-%d %H:%M:%S\"))]\n",
    "\n",
    "df_intermediate = df_intermediate.explode(\"floorPlans\")\n",
    "df_intermediate = df_intermediate.replace({'postType': {'R': 'Rent',\n",
    "                                                        'S': 'Sale'},\n",
    "                                            'unitType': {'AP': 'Apartment',\n",
    "                                                        'HS': 'House'}})\n",
    "\n",
    "df_clean = df_intermediate.rename(columns=dict_column_mapping)\n",
    "load_data_to_table(\"project-diver\",\"project-diver\",\"dbo.property_transaction\",df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "\n",
    "table_id = \".\".join([\"project-diver\",\"dbo.property_transaction\"])\n",
    "\n",
    "load_job = client.load_table_from_dataframe(\n",
    "    dataframe= df_clean, destination=table_id\n",
    ")  # Make an API request.\n",
    "\n",
    "result = load_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n"
     ]
    }
   ],
   "source": [
    "print(result.output_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
